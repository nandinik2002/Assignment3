### **Overall Comparison**
_All of the test files seems to have a worse compression ratio when compressing using the LZW algorithm implementation with 12-bit codewords. This is because with a 12-bit codeword size, the maximum number of unique codes would be 2^12 (4096). However, when we look at the implementation of the LZW algorithm that uses 9-bit through 16-bit codewords, the number of unique codes would be 512 through 65,536. This means the second implementation has a larger dictionary size and can represent a larger number of codewords, which explains why the 12-bit algorithm has a worse compression ratio.
Overall, the 'ideal' compression algorithm would be the Compress.exe algorithm while the worst would be the 12-bit codeword algorithm implementation._


### **Provided implementation that uses 12-bit codewords before any modifications**
_For this implementation the files with the best compression ratios were the .txt files and wacky.bmp. Wacky.bmp compressed the best because the image is mostly white space with words in the same color. There is an obvious pattern, which is why even with a static bit sized limit on the codewords, its able to be compressed really well. Although not as well as wacky.bmp, all of the .txt files are able to be compressed well too. This is because .txt files tend to have many patterns as they reuse characters and words frequently. The files with the worst compression ratios were, edit.exe, frosty.jpg, Lego-big.gif, winnt256.bmp._ 




### **Modified implementation with codeword size going from 9 bits to 16 bits without codebook reset**
_For this implementation, the files with the best compression ratios were, all of the .bmp files and all of the .txt files.




### **Modified implementation with codeword size going from 9 bits to 16 bits with codebook reset**





### **Predefined compress.exe program**
