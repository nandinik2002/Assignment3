| Column 1 | Column 2 | Column 3 |
|---|---|---|
| Data 1 | Data 2 | Data 3 |
| Data 4 | Data 5 | Data 6 |



### **Overall Comparison**
_All of the test files seems to have a worse compression ratios when compressing using the LZW algorithm implementation with 12-bit codewords. This is because with a 12-bit codeword size, the maximum number of unique codes would be 2^12 (4096). However, when we look at the implementation of the LZW algorithm that uses 9-bit through 16-bit codewords, the number of unique codes would be 512 through 65,536. This means the second implementation has a larger dictionary size and can represent a larger number of codewords, which explains why the 12-bit algorithm has a worse compression ratio.
Overall, the wacky.bmp file tends to always significantly have a better compression ratio, becasue the file is mostly white space, which compresses really well since it's a significant pattern and would only use 1 white pixel most of the time. The Lego-big.gif and frosty.jpg files consistently had bad compression ratios. This is because a .gif could have multiple frames per second that we would have to try to compress, which is close to impossible. Similarly, looking at the .jpg file, there are no patterns and when zooming in, we can notice that although we may see a blue color, there are a significant number of shades used to create that portion of the image. The range of color pixels used greatly vary, making it not able to be compressed any further since there seems to be not much of a pattern. 
The 'ideal' compression algorithm would be the Compress.exe algorithm while the worst would be the 12-bit codeword algorithm implementation._



### **Provided implementation that uses 12-bit codewords before any modifications**
_For this implementation the files with the best compression ratios were the .txt files and wacky.bmp. Wacky.bmp compressed the best because the image is mostly white space with words in the same color. There is an obvious pattern, which is why even with a static bit sized limit on the codewords, its able to be compressed really well. Although not as well as wacky.bmp, all of the .txt files are able to be compressed well too. This is because .txt files tend to have many patterns as they reuse characters and words frequently. The files with the worst compression ratios were, edit.exe, frosty.jpg, Lego-big.gif, winnt256.bmp._ 




### **Modified implementation with codeword size going from 9 bits to 16 bits without codebook reset**
_We tend to not use codebook reset when we are looking at a file or group of files that stay consistent in their pattern or type of pattern. For example, we wouldn't use reset for .txt files, since they stay consistent with the letters and words, and in general the type of data, they display. However, we would use codebook reset in instances where the pattern differs. For example, we would prefer to use reset for .tar files, since they consist of a variety of different file types that could go from .jpg to .txt to .doc, like for this assignment. Although .tar files are already compressed, compressing them with any algorithm or implementation would lead to them having a great compression ratio, however, they get the most ideal ratio when using codebook reset, as we can see in the table above.
For this implementation, the files with the best compression ratios were, all of the .bmp files and all of the .txt files. The .tar files compress well too, as expected, since they compress well in any implementation. The files that have a bad compression ratio are, as expected, Lego-big.gif and frosty.jpg._




### **Modified implementation with codeword size going from 9 bits to 16 bits with codebook reset**
_We tend to not use codebook reset when we are looking at a file or group of files that stay consistent in their pattern or type of pattern. For example, we wouldn't use reset for .txt files, since they stay consistent with the letters and words, and in general the type of data, they display. However, we would use codebook reset in instances where the pattern differs. For example, we would prefer to use reset for .tar files, since they consist of a variety of different file types that could go from .jpg to .txt to .doc, like for this assignment. Although .tar files are already compressed, compressing them with any algorithm or implementation would lead to them having a great compression ratio, however, they get the most ideal ratio when using codebook reset, as we can see in the table above.
For this implementation, the files with the best compression ratios, as we can guess based on the explanation above, are assig2.doc, .txt files, .tar files and .bmp files. Assig2.doc contains different forms of data like graph, words, and text boxes, which could be why the algorithm with codebook reset was able to compress this file well. Although .txt files do only have text and should be compressed without reset, this form of compression still worked well, although not as well as without reset. As stated before, .tar files compressed well with reset because it contains a variety of files with texts, images, and texts, especially all.tar. The files that didnt compress well, and even expanded, are, frosty.jpg and Lego-big.gif. As stated before, no matter what compression algorithm we use these files would be difficult to compress at all because of their properties._


different paridims of file types 

### **Predefined compress.exe program**
_For this implementation, the files that had the best compression ratios were .tar files, assig2.doc, .txt files, and .bmp files. The files that had bad compression ratios, below 1, which means they expanded during compression, where Lego-big.gif and frosty.gif._
